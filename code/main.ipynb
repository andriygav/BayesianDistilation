{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "main.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "8gFa8XEB6ATP",
        "zHl3U2xrrpf6",
        "z_77Efec6BnU"
      ],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKxoz2K5Yo1c"
      },
      "source": [
        "import torch\n",
        "\n",
        "from torchvision import datasets\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-_0AJ4k6vkX"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gFa8XEB6ATP"
      },
      "source": [
        "# Модель"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SCxl7bUYo1t"
      },
      "source": [
        "class BayesianNetworkFNN(torch.nn.Module):\n",
        "    @property\n",
        "    def device(self):\n",
        "        return next(self.parameters()).device\n",
        "    \n",
        "    def __init__(self, \n",
        "                 input_dim=10, \n",
        "                 output_dim=1, \n",
        "                 layers=[],\n",
        "                 mu_prior=None,\n",
        "                 sigma_prior=None):\n",
        "        r'''\n",
        "        Initialise Neural Network for Bayesian inference\n",
        "\n",
        "        :param input_dim: size of feature space\n",
        "        :type input_dim: int\n",
        "        :param output_dim: size of target space\n",
        "        :type output_dim: int\n",
        "        :param layers: sizes of each layer for fully connected neural network\n",
        "        :type layers: list[int]\n",
        "        :param mu_prior: parameter of prior for :math:`\\mathsf(E)w`\n",
        "        :type mu_prior: array\n",
        "        :param sigma_prior: parameter of prior for :math:`\\mathsf(V)w`\n",
        "        :type sigma_prior: array\n",
        "        '''\n",
        "        super(BayesianNetworkFNN, self).__init__()\n",
        "\n",
        "        layers = [input_dim] + \\\n",
        "                 [layer for layer in layers if layer != 0] + \\\n",
        "                 [output_dim]\n",
        "\n",
        "        self.network = torch.nn.Sequential()\n",
        "        for i in range(1, len(layers)):\n",
        "            self.network.add_module('layer{}'.format(i), \n",
        "                                    torch.nn.Linear(layers[i-1], \n",
        "                                                    layers[i]))\n",
        "            if i != len(layers) - 1:\n",
        "                self.network.add_module('relu{}'.format(i), \n",
        "                                        torch.nn.LeakyReLU())\n",
        "        weight = self.get_weight(self.network)\n",
        "        \n",
        "        self.log_sigma_posterior = torch.nn.Parameter(\n",
        "            torch.zeros_like(weight).float(), requires_grad=True)\n",
        "        self.log_sigma_prior = torch.nn.Parameter(\n",
        "            torch.zeros_like(weight).float(), requires_grad=True)\n",
        "        self.mu_prior = torch.nn.Parameter(\n",
        "            weight.data, requires_grad=True)\n",
        "        \n",
        "        if sigma_prior is not None:\n",
        "            self.log_sigma_prior = torch.nn.Parameter(\n",
        "                torch.log(torch.tensor(sigma_prior).float()), requires_grad=True)\n",
        "\n",
        "        if mu_prior is not None:\n",
        "            self.mu_prior = torch.nn.Parameter(\n",
        "                torch.tensor(mu_prior).float(), requires_grad=True)\n",
        "            \n",
        "            self.set_weight(self.network, weight, self.mu_prior.data)\n",
        "\n",
        "        assert len(self.log_sigma_prior) == len(weight), \\\n",
        "          'length of sigma_prior and weight must bu equels but {} != {}'.format(\n",
        "              len(self.log_sigma_prior), len(weight)\n",
        "          )\n",
        "        assert len(self.mu_prior) == len(weight), \\\n",
        "          'length of mu_prior and weight must bu equels but {} != {}'.format(\n",
        "              len(self.mu_prior), len(weight)\n",
        "          )\n",
        "\n",
        "    def posterior_parameters(self):\n",
        "        yield self.log_sigma_posterior\n",
        "        for param in self.network.parameters():\n",
        "            yield param\n",
        "\n",
        "    def prior_parameters(self):\n",
        "        yield self.log_sigma_prior\n",
        "        yield self.mu_prior\n",
        "\n",
        "    @staticmethod  \n",
        "    def set_weight(seq, weight, new_weight):\n",
        "        assert len(weight) == len(new_weight)\n",
        "\n",
        "        bias = 0\n",
        "        for param in seq.parameters():\n",
        "            param_size = torch.tensor(param.size()).prod()\n",
        "            param.data = new_weight[bias:bias+param_size].view_as(param)\n",
        "            \n",
        "            bias += param_size\n",
        "\n",
        "    @property\n",
        "    def weight(self):\n",
        "        return self.get_weight(self.network)\n",
        "\n",
        "    @staticmethod\n",
        "    def get_weight(seq, requires_grad=True):\n",
        "        weight = None\n",
        "        if requires_grad:\n",
        "            parameters = []\n",
        "            for param in seq.parameters():\n",
        "                parameters.append(param.view(-1))\n",
        "\n",
        "            weight = torch.cat(parameters)\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                parameters = []\n",
        "                for param in seq.parameters():\n",
        "                    parameters.append(param.view(-1))\n",
        "\n",
        "                weight = torch.cat(parameters)\n",
        "        return weight\n",
        "\n",
        "    def forward(self, x_batch):\n",
        "        r'''\n",
        "        Model inference for one batch\n",
        "\n",
        "        :param x_batch: \n",
        "            input tensor of shape `batch_size` :math:`\\times` `input_dim`\n",
        "        :type x_batch: Tensor\n",
        "        :return: \n",
        "            output tensor of shape `batch_size` :math:`\\times` `output_dim`\n",
        "        :rtype: Tensor\n",
        "        '''\n",
        "\n",
        "        return self.network(x_batch)\n",
        "\n",
        "    def _D_KL_loss(self):\n",
        "        r'''\n",
        "        The method return KL divergence between prior and posterior. \n",
        "        The distributions are assumed to be normal.\n",
        "        '''\n",
        "\n",
        "        mu_posterior = self.weight\n",
        "\n",
        "        D_KL_1 = 0.5*torch.exp(self.log_sigma_posterior \\\n",
        "                               -self.log_sigma_prior).sum()\n",
        "        D_KL_2 = (0.5*torch.exp(-self.log_sigma_prior) \\\n",
        "                     *(self.mu_prior-mu_posterior)**2).sum()\n",
        "        D_KL_3 = 0.5*(self.log_sigma_prior.sum() \\\n",
        "                      - self.log_sigma_posterior.sum())\n",
        "        D_KL_4 = -1*0.5*len(mu_posterior)\n",
        "\n",
        "        return D_KL_1 + D_KL_2 + D_KL_3 + D_KL_4\n",
        "\n",
        "    def loss(self, likelihood, alpha=1.):\n",
        "        r'''\n",
        "        Compute completed loss for bayesian model.\n",
        "        :param likelihood: data likelihood\n",
        "        :type likelihood: Tensor\n",
        "        '''\n",
        "        return alpha*self._D_KL_loss() - likelihood\n",
        "\n",
        "\n",
        "    def fit(self, \n",
        "            train_dataset,\n",
        "            loss_function=torch.nn.MSELoss,\n",
        "            optim_function=torch.optim.Adam,\n",
        "            epochs=100,\n",
        "            lr=0.001,\n",
        "            hyp_lr=0.00001, \n",
        "            batch_size=8,\n",
        "            callback=None):\n",
        "      \n",
        "        optimiser_posterior = optim_function(self.posterior_parameters(), lr=lr)\n",
        "        optimiser_prior = optim_function(self.prior_parameters(), lr=hyp_lr)\n",
        "        loss_funct = loss_function()\n",
        "      \n",
        "        dataloader = torch.utils.data.DataLoader(\n",
        "            train_dataset, batch_size=batch_size, pin_memory=True, shuffle=True)\n",
        "        \n",
        "        for epoch in tqdm(range(epochs)):\n",
        "            total_loss = 0\n",
        "            for x_batch, y_batch in dataloader:\n",
        "                x_batch = x_batch.to(self.device)\n",
        "                y_batch = y_batch.to(self.device)\n",
        "\n",
        "                optimiser_posterior.zero_grad()\n",
        "                output = self(x_batch)\n",
        "\n",
        "                if len(y_batch.shape) > 1:\n",
        "                    likelihood_1 = -1*loss_funct(\n",
        "                        output.transpose(1, -1), \n",
        "                        y_batch.transpose(1, -1))/len(x_batch)\n",
        "                else:\n",
        "                    likelihood_1 = -1*loss_funct(\n",
        "                        output, \n",
        "                        y_batch)/len(x_batch)\n",
        "                loss_1 = self.loss(likelihood_1)\n",
        "\n",
        "                loss_1.backward()\n",
        "                optimiser_posterior.step()\n",
        "\n",
        "\n",
        "                optimiser_prior.zero_grad()\n",
        "                output = self(x_batch)\n",
        "\n",
        "                if len(y_batch.shape) > 1:\n",
        "                    likelihood_2 = -1*loss_funct(\n",
        "                        output.transpose(1, -1), \n",
        "                        y_batch.transpose(1, -1))/len(x_batch)\n",
        "                else:\n",
        "                    likelihood_2 = -1*loss_funct(\n",
        "                        output, \n",
        "                        y_batch)/len(x_batch)\n",
        "                loss_2 = self.loss(likelihood_2)\n",
        "\n",
        "                loss_2.backward()\n",
        "                optimiser_prior.step()\n",
        "\n",
        "                if callback is not None:\n",
        "                    callback(self, likelihood_1, likelihood_2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHl3U2xrrpf6"
      },
      "source": [
        "# CallBack"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZ9ll030rrAr"
      },
      "source": [
        "class callback():\n",
        "    def __init__(self, writer, dataset, \n",
        "                 loss_function=torch.nn.MSELoss, \n",
        "                 delimeter = 100, \n",
        "                 batch_size = 64):\n",
        "        self.step = 0\n",
        "        self.writer = writer\n",
        "        self.delimeter = delimeter\n",
        "        self.loss_function = loss_function\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.dataset = dataset\n",
        "\n",
        "    def forward(self, model, *loss):\n",
        "        self.step += 1\n",
        "        for i, ls in enumerate(loss):\n",
        "            self.writer.add_scalar('TRAIN/likelihood_{}'.format(i), ls, self.step)\n",
        "        loss_funct = self.loss_function()\n",
        "        if self.step % self.delimeter == 0:\n",
        "\n",
        "            batch_generator = torch.utils.data.DataLoader(\n",
        "                dataset = self.dataset, batch_size=self.batch_size)\n",
        "            \n",
        "            test_loss = 0\n",
        "            with torch.no_grad():\n",
        "                for it, (x_batch, y_batch) in enumerate(batch_generator):\n",
        "                    x_batch = x_batch.to(model.device)\n",
        "                    y_batch = y_batch.to(model.device)\n",
        "                    \n",
        "                    output = model(x_batch)\n",
        "\n",
        "                    if len(y_batch.shape) > 1:\n",
        "                        test_loss += -1*loss_funct(\n",
        "                            output.transpose(1, -1), \n",
        "                            y_batch.transpose(1, -1)).cpu().item()*len(x_batch)\n",
        "                    else:\n",
        "                        test_loss += -1*loss_funct(\n",
        "                            output, \n",
        "                            y_batch).cpu().item()*len(x_batch)\n",
        "            \n",
        "            test_loss /= len(self.dataset)\n",
        "            \n",
        "            self.writer.add_scalar('TEST/likelihood', test_loss, self.step)\n",
        "\n",
        "            self.writer.add_scalar('VARIATION/D_KL', \n",
        "                                   model._D_KL_loss(), self.step)\n",
        "            \n",
        "            writer.add_histogram('weight', model.weight, self.step)\n",
        "\n",
        "\n",
        "    def __call__(self, model, *loss):\n",
        "        return self.forward(model, *loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nJ11s9NG_Cq"
      },
      "source": [
        "# Синтетика"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r63nxQ6trdIM"
      },
      "source": [
        "## Tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sbbDDkPrfP7"
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir experiment/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_77Efec6BnU"
      },
      "source": [
        "## Данные"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NKIE-p4n4vl"
      },
      "source": [
        "np.random.seed(0)\n",
        "\n",
        "l = 1024\n",
        "n = 10\n",
        "X = np.random.randn(l, n)\n",
        "w = np.random.randn(n)\n",
        "\n",
        "y = X@w + 0.1*np.random.randn(l)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VoHLBnML5CyR"
      },
      "source": [
        "X_train_tr = torch.tensor(X[:900]).float()\n",
        "X_test_tr = torch.tensor(X[900:]).float()\n",
        "\n",
        "y_train_tr = torch.tensor(y[:900]).view(-1, 1).float()\n",
        "y_test_tr = torch.tensor(y[900:]).view(-1, 1).float()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3Euwip85DSi"
      },
      "source": [
        "train_dataset = torch.utils.data.TensorDataset(X_train_tr, y_train_tr)\n",
        "test_dataset = torch.utils.data.TensorDataset(X_test_tr, y_test_tr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "en23y2RtIPy1"
      },
      "source": [
        "## Параметры обучения"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLjHS_K58g9N"
      },
      "source": [
        "epochs = 150\n",
        "delimeter = 100\n",
        "teacher_3W_layers = [100, 50]\n",
        "student_3W_layers = [10, 10]\n",
        "# хоть и оставляем первый слой, а удаляем второй, но в параметрах остается \n",
        "# минимум из двух размерностей матрици параметров\n",
        "student_2W_layers = [50, 0]\n",
        "input_dim = 10\n",
        "output_dim = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YDtUx3-tyKL"
      },
      "source": [
        "## Обучение"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eBASOqfjiK7"
      },
      "source": [
        "teacher = BayesianNetworkFNN(input_dim=input_dim, \n",
        "                             output_dim=output_dim, \n",
        "                             layers=teacher_3W_layers)\n",
        "_ = teacher.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocCTzdJNQ2Dm"
      },
      "source": [
        "writer = SummaryWriter(log_dir = 'experiment/3W/teacher')\n",
        "\n",
        "call = callback(writer, test_dataset, delimeter=delimeter)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cegpo_BztCli"
      },
      "source": [
        "teacher.fit(train_dataset, callback=call, epochs=epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_aH6d0OtJXa"
      },
      "source": [
        "student = BayesianNetworkFNN(input_dim=input_dim, \n",
        "                             output_dim=output_dim, \n",
        "                             layers=student_3W_layers)\n",
        "_ = student.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0P8pp2gfxZ48"
      },
      "source": [
        "writer = SummaryWriter(log_dir = 'experiment/3W/student')\n",
        "\n",
        "call = callback(writer, test_dataset, delimeter=delimeter)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNZZpQlczwJQ"
      },
      "source": [
        "student.fit(train_dataset, callback=call, epochs=epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZH9ZLxVCWCm6"
      },
      "source": [
        "student = BayesianNetworkFNN(input_dim=input_dim, \n",
        "                             output_dim=output_dim, \n",
        "                             layers=student_2W_layers)\n",
        "_ = student.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4mufIPEV-EV"
      },
      "source": [
        "writer = SummaryWriter(log_dir = 'experiment/2W/student')\n",
        "\n",
        "call = callback(writer, test_dataset, delimeter=delimeter)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJVBpdxxV_FP"
      },
      "source": [
        "student.fit(train_dataset, callback=call, epochs=epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1Dyvbwh07lB"
      },
      "source": [
        "## Дистиляция"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkDLMZXxVDHE"
      },
      "source": [
        "### Удаление нейронов\n",
        "Так как ковариация отсутсвует, то в данном случае формула превращения параметров будет тривиальная\n",
        "$$\n",
        "p(u) = \\mathcal{N}\\bigr(\\mu_{posterior}', \\Sigma_{posterior}'\\bigr)\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjaT-NvV1wZl"
      },
      "source": [
        "teacher_3W_layers_distil=[input_dim] + teacher_3W_layers + [output_dim]\n",
        "student_3W_layers_distil=[input_dim] + student_3W_layers + [output_dim]\n",
        "\n",
        "np.random.seed(0)\n",
        "preference = [np.random.choice(np.arange(0, teacher_layer_size), \n",
        "                               size=student_layer_size, \n",
        "                               replace=False ).tolist() \\\n",
        "              for teacher_layer_size, student_layer_size in zip(\n",
        "                  teacher_3W_layers_distil, student_3W_layers_distil)]\n",
        " \n",
        "weight_mask = []\n",
        "for id in range(1, len(teacher_3W_layers_distil)):\n",
        "    layer_mask = np.zeros(\n",
        "        shape=[teacher_3W_layers_distil[id-1], teacher_3W_layers_distil[id]])\n",
        "    \n",
        "    bias_mask = np.zeros(teacher_3W_layers_distil[id])\n",
        "\n",
        "    for pref_neuron_id in preference[id-1]:\n",
        "        for cur_neuron_id in preference[id]:\n",
        "            layer_mask[pref_neuron_id, cur_neuron_id] = 1\n",
        "\n",
        "    for cur_neuron_id in preference[id]:\n",
        "        bias_mask[cur_neuron_id] = 1\n",
        "    \n",
        "    weight_mask += layer_mask.reshape(-1).tolist()\n",
        "    weight_mask += bias_mask.reshape(-1).tolist()\n",
        "\n",
        "weight_mask = np.array(weight_mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2TbblXxE6bCf"
      },
      "source": [
        "mu_prior = teacher.weight[weight_mask == 1].detach().cpu().numpy().tolist()\n",
        "sigma_prior = torch.exp(\n",
        "    teacher.log_sigma_prior)[weight_mask == 1].detach().cpu().numpy().tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KP_1vt1x6sWI"
      },
      "source": [
        "student = BayesianNetworkFNN(input_dim=input_dim, \n",
        "                             output_dim=output_dim, \n",
        "                             layers=student_3W_layers, \n",
        "                             mu_prior=mu_prior)\n",
        "_ = student.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQUsiTmM7ImI"
      },
      "source": [
        "writer = SummaryWriter(log_dir = 'experiment/3W/distil_student')\n",
        "\n",
        "call = callback(writer, test_dataset, delimeter=delimeter)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mT4E-w5K7KwX"
      },
      "source": [
        "student.fit(train_dataset, callback=call, epochs=epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kusM_2L7M7T"
      },
      "source": [
        "student = BayesianNetworkFNN(input_dim=input_dim, \n",
        "                             output_dim=output_dim, \n",
        "                             layers=student_3W_layers, \n",
        "                             mu_prior=mu_prior, \n",
        "                             sigma_prior=sigma_prior)\n",
        "_ = student.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4aXEue0J8CAD"
      },
      "source": [
        "writer = SummaryWriter(log_dir = 'experiment/3W/distil_student_all')\n",
        "\n",
        "call = callback(writer, test_dataset, delimeter=delimeter)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hauSB2Ss8Dd1"
      },
      "source": [
        "student.fit(train_dataset, callback=call, epochs=epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKhVx0KwVIAO"
      },
      "source": [
        "### Удаление слоя\n",
        "\n",
        "Так как ковариация отсутсвует, то в данном случае формула превращения параметров будет тривиальная\n",
        "$$\n",
        "p(u) = \\mathcal{N}\\bigr(\\mu_{posterior}', \\Sigma_{posterior}'\\bigr)\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayGmQv6KVJg2"
      },
      "source": [
        "# сначала выбираем какой слой убираем \n",
        "#      (в нашем случае я убираю вторую матрицу W_2)\n",
        "# опять таки так как ковариации нет, то просто удаляем вторую матрицу \n",
        "# и берем подмножетво строк первой матрицы\n",
        "teacher_3W_layers_distil=[input_dim] + teacher_3W_layers + [output_dim]\n",
        "student_2W_layers_distil=[input_dim] + student_2W_layers + [output_dim]\n",
        "\n",
        "np.random.seed(0)\n",
        "preference = [np.random.choice(np.arange(0, teacher_layer_size), \n",
        "                               size=student_layer_size, \n",
        "                               replace=False ).tolist() \\\n",
        "              for teacher_layer_size, student_layer_size in zip(\n",
        "                  teacher_3W_layers_distil, student_2W_layers_distil)]\n",
        " \n",
        "weight_mask = []\n",
        "prev_id = 0\n",
        "for id in range(1, len(teacher_3W_layers_distil)):\n",
        "    layer_mask = np.zeros(\n",
        "        shape=[teacher_3W_layers_distil[id-1], teacher_3W_layers_distil[id]])\n",
        "    \n",
        "    bias_mask = np.zeros(teacher_3W_layers_distil[id])\n",
        "\n",
        "    prev_pref = preference[id-1]\n",
        "    if not prev_pref:\n",
        "        prev_pref = np.arange(0, teacher_3W_layers_distil[id-1])\n",
        "    for pref_neuron_id in prev_pref:\n",
        "        for cur_neuron_id in preference[id]:\n",
        "            layer_mask[pref_neuron_id, cur_neuron_id] = 1\n",
        "\n",
        "    for cur_neuron_id in preference[id]:\n",
        "        bias_mask[cur_neuron_id] = 1\n",
        "        \n",
        "    weight_mask += layer_mask.reshape(-1).tolist()\n",
        "    weight_mask += bias_mask.reshape(-1).tolist()\n",
        "\n",
        "weight_mask = np.array(weight_mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NCdQETElXJM"
      },
      "source": [
        "student_2W_layers_distil, teacher_3W_layers_distil"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "et5Wr-GPj4Jd"
      },
      "source": [
        "mu_prior = teacher.weight[weight_mask == 1].detach().cpu().numpy().tolist()\n",
        "sigma_prior = torch.exp(\n",
        "    teacher.log_sigma_prior)[weight_mask == 1].detach().cpu().numpy().tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4skVzyvkLnG"
      },
      "source": [
        "student = BayesianNetworkFNN(input_dim=input_dim, \n",
        "                             output_dim=output_dim, \n",
        "                             layers=student_2W_layers, \n",
        "                             mu_prior=mu_prior)\n",
        "_ = student.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEIyF_BwmFmj"
      },
      "source": [
        "writer = SummaryWriter(log_dir = 'experiment/2W/distil_student')\n",
        "\n",
        "call = callback(writer, test_dataset, delimeter=delimeter)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YJ4NHJ6mFmk"
      },
      "source": [
        "student.fit(train_dataset, callback=call, epochs=epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioYfmPlNmKw1"
      },
      "source": [
        "student = BayesianNetworkFNN(input_dim=input_dim, \n",
        "                             output_dim=output_dim, \n",
        "                             layers=student_2W_layers, \n",
        "                             mu_prior=mu_prior, \n",
        "                             sigma_prior=sigma_prior)\n",
        "_ = student.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJek6M7JmKw2"
      },
      "source": [
        "writer = SummaryWriter(log_dir = 'experiment/2W/distil_student_all')\n",
        "\n",
        "call = callback(writer, test_dataset, delimeter=delimeter)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHaMvPftmKw3"
      },
      "source": [
        "student.fit(train_dataset, callback=call, epochs=epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0d7g284HJ5c"
      },
      "source": [
        "# FashionMnist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pqvee9CzHXs3"
      },
      "source": [
        "## Tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7OOFmpeHXfe"
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir fashionmnist/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pP3uf_4oH5UW"
      },
      "source": [
        "## Данные"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-PQDG4WHMTT"
      },
      "source": [
        "MNIST = datasets.FashionMNIST('../data/', train=True, download=True, transform=None)\n",
        "train_dataset = torch.utils.data.TensorDataset(MNIST.data.view(-1, 28 * 28).float() / 255, MNIST.targets)\n",
        "\n",
        "MNIST = datasets.FashionMNIST('../data/', train=False, download=True, transform=None)\n",
        "test_dataset = torch.utils.data.TensorDataset(MNIST.data.view(-1, 28 * 28).float() / 255, MNIST.targets)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcE7LxFvIqan"
      },
      "source": [
        "## Параметры обучения"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QS24XY3ZIB6h"
      },
      "source": [
        "epochs = 30\n",
        "delimeter = 100\n",
        "teacher_3W_layers = [100, 50]\n",
        "student_3W_layers = [10, 10]\n",
        "# хоть и оставляем первый слой, а удаляем второй, но в параметрах остается \n",
        "# минимум из двух размерностей матрици параметров\n",
        "student_2W_layers = [50, 0]\n",
        "input_dim = 784\n",
        "output_dim = 10\n",
        "batch_size = 64"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HidoKvfItb9"
      },
      "source": [
        "## Обучение"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRNdtVOdIvF-"
      },
      "source": [
        "teacher = BayesianNetworkFNN(input_dim=input_dim, \n",
        "                             output_dim=output_dim, \n",
        "                             layers=teacher_3W_layers)\n",
        "_ = teacher.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbcVnI2lI4SJ"
      },
      "source": [
        "writer = SummaryWriter(log_dir = 'fashionmnist/3W/teacher')\n",
        "\n",
        "call = callback(writer, test_dataset, \n",
        "                delimeter=delimeter, loss_function=torch.nn.CrossEntropyLoss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-vCLxTAI-ER"
      },
      "source": [
        "teacher.fit(train_dataset, \n",
        "            callback=call, \n",
        "            epochs=epochs,\n",
        "            batch_size=batch_size,\n",
        "            loss_function=torch.nn.CrossEntropyLoss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dongm_8aMmS0"
      },
      "source": [
        "student = BayesianNetworkFNN(input_dim=input_dim, \n",
        "                             output_dim=output_dim, \n",
        "                             layers=student_3W_layers)\n",
        "_ = student.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "op4IynJgMmS5"
      },
      "source": [
        "writer = SummaryWriter(log_dir = 'fashionmnist/3W/student')\n",
        "\n",
        "call = callback(writer, test_dataset, \n",
        "                delimeter=delimeter, loss_function=torch.nn.CrossEntropyLoss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOu5-JEbMmS5"
      },
      "source": [
        "student.fit(train_dataset, \n",
        "            callback=call, \n",
        "            epochs=epochs,\n",
        "            batch_size=batch_size,\n",
        "            loss_function=torch.nn.CrossEntropyLoss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwToS0_pMmS5"
      },
      "source": [
        "student = BayesianNetworkFNN(input_dim=input_dim, \n",
        "                             output_dim=output_dim, \n",
        "                             layers=student_2W_layers)\n",
        "_ = student.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfEzFZYbMmS6"
      },
      "source": [
        "writer = SummaryWriter(log_dir = 'fashionmnist/2W/student')\n",
        "\n",
        "call = callback(writer, test_dataset, \n",
        "                delimeter=delimeter, loss_function=torch.nn.CrossEntropyLoss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNS2eDb-MmS6"
      },
      "source": [
        "student.fit(train_dataset, \n",
        "            callback=call, \n",
        "            epochs=epochs,\n",
        "            batch_size=batch_size,\n",
        "            loss_function=torch.nn.CrossEntropyLoss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCEApSWVN0gt"
      },
      "source": [
        "## Дистиляция"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MABJhENoN0gu"
      },
      "source": [
        "### Удаление нейронов\n",
        "Так как ковариация отсутсвует, то в данном случае формула превращения параметров будет тривиальная\n",
        "$$\n",
        "p(u) = \\mathcal{N}\\bigr(\\mu_{posterior}', \\Sigma_{posterior}'\\bigr)\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmeESkkEN0gv"
      },
      "source": [
        "teacher_3W_layers_distil=[input_dim] + teacher_3W_layers + [output_dim]\n",
        "student_3W_layers_distil=[input_dim] + student_3W_layers + [output_dim]\n",
        "\n",
        "np.random.seed(0)\n",
        "preference = [np.random.choice(np.arange(0, teacher_layer_size), \n",
        "                               size=student_layer_size, \n",
        "                               replace=False ).tolist() \\\n",
        "              for teacher_layer_size, student_layer_size in zip(\n",
        "                  teacher_3W_layers_distil, student_3W_layers_distil)]\n",
        " \n",
        "weight_mask = []\n",
        "for id in range(1, len(teacher_3W_layers_distil)):\n",
        "    layer_mask = np.zeros(\n",
        "        shape=[teacher_3W_layers_distil[id-1], teacher_3W_layers_distil[id]])\n",
        "    \n",
        "    bias_mask = np.zeros(teacher_3W_layers_distil[id])\n",
        "\n",
        "    for pref_neuron_id in preference[id-1]:\n",
        "        for cur_neuron_id in preference[id]:\n",
        "            layer_mask[pref_neuron_id, cur_neuron_id] = 1\n",
        "\n",
        "    for cur_neuron_id in preference[id]:\n",
        "        bias_mask[cur_neuron_id] = 1\n",
        "    \n",
        "    weight_mask += layer_mask.reshape(-1).tolist()\n",
        "    weight_mask += bias_mask.reshape(-1).tolist()\n",
        "\n",
        "weight_mask = np.array(weight_mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWF7ZxK0N0gv"
      },
      "source": [
        "mu_prior = teacher.weight[weight_mask == 1].detach().cpu().numpy().tolist()\n",
        "sigma_prior = torch.exp(\n",
        "    teacher.log_sigma_prior)[weight_mask == 1].detach().cpu().numpy().tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aw4FbIluN0gw"
      },
      "source": [
        "student = BayesianNetworkFNN(input_dim=input_dim, \n",
        "                             output_dim=output_dim, \n",
        "                             layers=student_3W_layers, \n",
        "                             mu_prior=mu_prior)\n",
        "_ = student.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pI10n8pON0gw"
      },
      "source": [
        "writer = SummaryWriter(log_dir = 'fashionmnist/3W/distil_student')\n",
        "\n",
        "call = callback(writer, test_dataset, \n",
        "                delimeter=delimeter, loss_function=torch.nn.CrossEntropyLoss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXf8XyzTN0gx"
      },
      "source": [
        "student.fit(train_dataset, \n",
        "            callback=call, \n",
        "            epochs=epochs,\n",
        "            batch_size=batch_size,\n",
        "            loss_function=torch.nn.CrossEntropyLoss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFLue2ZAN0gx"
      },
      "source": [
        "student = BayesianNetworkFNN(input_dim=input_dim, \n",
        "                             output_dim=output_dim, \n",
        "                             layers=student_3W_layers, \n",
        "                             mu_prior=mu_prior, \n",
        "                             sigma_prior=sigma_prior)\n",
        "_ = student.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Bx0kehxN0gy"
      },
      "source": [
        "writer = SummaryWriter(log_dir = 'fashionmnist/3W/distil_student_all')\n",
        "\n",
        "call = callback(writer, test_dataset, \n",
        "                delimeter=delimeter, loss_function=torch.nn.CrossEntropyLoss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-OFzNEzZN0gy"
      },
      "source": [
        "student.fit(train_dataset, \n",
        "            callback=call, \n",
        "            epochs=epochs,\n",
        "            batch_size=batch_size,\n",
        "            loss_function=torch.nn.CrossEntropyLoss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xdZF4AaN0gy"
      },
      "source": [
        "### Удаление слоя\n",
        "\n",
        "Так как ковариация отсутсвует, то в данном случае формула превращения параметров будет тривиальная\n",
        "$$\n",
        "p(u) = \\mathcal{N}\\bigr(\\mu_{posterior}', \\Sigma_{posterior}'\\bigr)\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2L1rmNgN0gz"
      },
      "source": [
        "# сначала выбираем какой слой убираем \n",
        "#      (в нашем случае я убираю вторую матрицу W_2)\n",
        "# опять таки так как ковариации нет, то просто удаляем вторую матрицу \n",
        "# и берем подмножетво строк первой матрицы\n",
        "teacher_3W_layers_distil=[input_dim] + teacher_3W_layers + [output_dim]\n",
        "student_2W_layers_distil=[input_dim] + student_2W_layers + [output_dim]\n",
        "\n",
        "np.random.seed(0)\n",
        "preference = [np.random.choice(np.arange(0, teacher_layer_size), \n",
        "                               size=student_layer_size, \n",
        "                               replace=False ).tolist() \\\n",
        "              for teacher_layer_size, student_layer_size in zip(\n",
        "                  teacher_3W_layers_distil, student_2W_layers_distil)]\n",
        " \n",
        "weight_mask = []\n",
        "prev_id = 0\n",
        "for id in range(1, len(teacher_3W_layers_distil)):\n",
        "    layer_mask = np.zeros(\n",
        "        shape=[teacher_3W_layers_distil[id-1], teacher_3W_layers_distil[id]])\n",
        "    \n",
        "    bias_mask = np.zeros(teacher_3W_layers_distil[id])\n",
        "\n",
        "    prev_pref = preference[id-1]\n",
        "    if not prev_pref:\n",
        "        prev_pref = np.arange(0, teacher_3W_layers_distil[id-1])\n",
        "    for pref_neuron_id in prev_pref:\n",
        "        for cur_neuron_id in preference[id]:\n",
        "            layer_mask[pref_neuron_id, cur_neuron_id] = 1\n",
        "\n",
        "    for cur_neuron_id in preference[id]:\n",
        "        bias_mask[cur_neuron_id] = 1\n",
        "        \n",
        "    weight_mask += layer_mask.reshape(-1).tolist()\n",
        "    weight_mask += bias_mask.reshape(-1).tolist()\n",
        "\n",
        "weight_mask = np.array(weight_mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-23p9Sc2N0gz"
      },
      "source": [
        "student_2W_layers_distil, teacher_3W_layers_distil"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTdP0ooaN0gz"
      },
      "source": [
        "mu_prior = teacher.weight[weight_mask == 1].detach().cpu().numpy().tolist()\n",
        "sigma_prior = torch.exp(\n",
        "    teacher.log_sigma_prior)[weight_mask == 1].detach().cpu().numpy().tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKrw8Wb6N0gz"
      },
      "source": [
        "student = BayesianNetworkFNN(input_dim=input_dim, \n",
        "                             output_dim=output_dim, \n",
        "                             layers=student_2W_layers, \n",
        "                             mu_prior=mu_prior)\n",
        "_ = student.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PjN0rIunN0g0"
      },
      "source": [
        "writer = SummaryWriter(log_dir = 'fashionmnist/2W/distil_student')\n",
        "\n",
        "call = callback(writer, test_dataset, \n",
        "                delimeter=delimeter, loss_function=torch.nn.CrossEntropyLoss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMCiJ60oN0g0"
      },
      "source": [
        "student.fit(train_dataset, \n",
        "            callback=call, \n",
        "            epochs=epochs,\n",
        "            batch_size=batch_size,\n",
        "            loss_function=torch.nn.CrossEntropyLoss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzP770f_N0g0"
      },
      "source": [
        "student = BayesianNetworkFNN(input_dim=input_dim, \n",
        "                             output_dim=output_dim, \n",
        "                             layers=student_2W_layers, \n",
        "                             mu_prior=mu_prior, \n",
        "                             sigma_prior=sigma_prior)\n",
        "_ = student.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXybzY8AN0g0"
      },
      "source": [
        "writer = SummaryWriter(log_dir = 'fashionmnist/2W/distil_student_all')\n",
        "\n",
        "call = callback(writer, test_dataset, \n",
        "                delimeter=delimeter, loss_function=torch.nn.CrossEntropyLoss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfxp7MZQN0g0"
      },
      "source": [
        "student.fit(train_dataset, \n",
        "            callback=call, \n",
        "            epochs=epochs,\n",
        "            batch_size=batch_size,\n",
        "            loss_function=torch.nn.CrossEntropyLoss)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}